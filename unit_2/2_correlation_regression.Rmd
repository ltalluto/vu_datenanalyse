---
title: "Correlation and Regression"
author: "Gabriel Singer"
date: "15.01.2024"
output:
  slidy_presentation:
    theme: cerulean
    toc_depth: 2
    css: ../assets/rmd_style.css
    mathjax: https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js
---


```{r setup, include=FALSE}
## r code goes in to chunks, delimited like this. you can add one with the "Insert" menu
## This setup chunk should be used for defining the document defaults
## here, we set the default figure size, the comment character, and the type
## of graphics to use
knitr::opts_chunk$set(fig.width=5.5, fig.height=5.5, collapse = TRUE, comment = "##", dev="png", error=TRUE)

## this chunk us also a cood place to load any librarys you want
## and to store constants in the R environment
## here I save a set of 8 colours for figures
library(RColorBrewer)
cols = brewer.pal(8, "Set1")
library(mvtnorm)
library(ggplot2)
library(gridExtra)
## These two packages make table display nicer; you can omit them if you don't want
## to use tables
library(knitr)
library(kableExtra)
options(digits = 3)
set.seed(12345)
```







## The linear model

* Correlation assumes nothing about the *causal* nature of the relationship between x and y
* Often, we have a hypothesis that a variable is in some way **functionally dependent** on another

* A **simple linear regression** describes/tests the relationship between
   - the **independent variable** (x, aka predictor or input variable), and
   - the **dependent variable** (y, aka response or outcome)


$$
\begin{aligned}
\mathbb{E}(y|x) & = \hat{y} = \alpha + \beta x \\
 & \approx a + bx \\ \\
\end{aligned}
$$

$y$ is not perfectly predicted by $x$, so we must include an error (residual) term:

$$
\begin{aligned}
y_i & = \hat{y_i} + \epsilon_i  \\ 
& = a + bx_i + \epsilon_i\\ \\
\epsilon & \sim \mathcal{N}(0, s_{\epsilon})
\end{aligned}
$$

## Method of least squares

$$
\begin{aligned}
\hat{y_i} & = a + b x_i \\
y_i & = \hat{y_i} + \epsilon_i  \\ 
\epsilon & \sim \mathcal{N}(0, s_{\epsilon})
\end{aligned}
$$

We want to solve these equations for $a$ and $b$

The "best" $a$ and $b$ are the ones that draw a line that is closest to the most points (i.e., that minimizes $s_{\epsilon}$)

```{r echo = FALSE, fig.width=12}
# load penguin data
data(penguins, package = "palmerpenguins")
# convert to a data frame
penguins = as.data.frame(penguins)
# remove NAs
penguins = penguins[complete.cases(penguins),]

# subset for just Adelie penguins
penguins_ade = subset(penguins, species == "Adelie")

pars = coef(lm(flipper_length_mm ~ body_mass_g, data = penguins_ade))

par(mfrow = c(1, 3))
plot(penguins_ade$body_mass_g, penguins_ade$flipper_length_mm, pch=16, 
     xlab = "Adelie body mass (g)", 
     ylab = "Adelie flipper length (mm)", bty='n', main = expression(best~line))
abline(a = pars[1], b = pars[2], col='blue', lwd=2)

plot(penguins_ade$body_mass_g, penguins_ade$flipper_length_mm, pch=16, 
     xlab = "Adelie body mass (g)", 
     ylab = "Adelie flipper length (mm)", bty='n', 
     main = expression(bias:~bar(epsilon) != 0))
abline(a = pars[1] + 15, b = pars[2], col='blue', lwd=2)

plot(penguins_ade$body_mass_g, penguins_ade$flipper_length_mm, pch=16, 
     xlab = "Adelie body mass (g)", 
     ylab = "Adelie flipper length (mm)", bty='n', 
     main = expression(precision:~s[epsilon]~too~large))
abline(a =mean(penguins_ade$flipper_length_mm), b = 0, col='blue', lwd=2)

```


## Method of least squares

$$
s_{\epsilon} = \sqrt{\frac{\sum_{i=1}^n \left (y_i -\hat{y_i}\right )^2}{n-2}}
$$

* $n-2$ is constant w.r.t. $a$ and $b$
* Simpler to compute the **Error** (aka **residual**) **Sum of Squares**:

$$
\begin{aligned}
\mathrm{ESS} & = \sum_{i=1}^n \left (y_i -\hat{y_i}\right )^2 \\
& = \sum_{i=1}^n \left (y_i - a - bx_i \right )^2
\end{aligned}
$$

## Ordinary least squares estimation

Solving for the minimum ESS yields:

$$
\begin{aligned}
b & = \frac{\mathrm{cov_{xy}}}{s^2_x} \\ \\
  & = r_{xy}\frac{s_y}{s_x}\\ \\
a & = \bar{y} - b\bar{x}
\end{aligned}
$$

The parameters have standard errors:

$$
s_a = s_{\hat{y}}\sqrt{\frac{1}{n} + \frac{\bar{x}^2}{\sum{(x-\bar{x}}^2)}}
$$

$$
s_b = \frac{s_{\hat{y}}}{\sqrt{\sum{(x-\bar{x}}^2)}}
$$

## Significance tests

$\mathbf{H_0}$: The slope $\beta$ = 0 (i.e., no variation in $y$ is explained by variation in $x$)

The ratio of variance explained to residual variance follows an $F$ distribution



\begin{aligned}
F & = \frac{MS_{model}}{MS_{err}} \\

MS_{model} & = \sum_{i=1}^n \left ( \hat{y}_i - \bar{y}\right)^2 \\
MS_{err} & = \frac{\sum_{i=1}^n \left ( y_i - \hat{y}_i \right)^2}{n-1}
\end{aligned}

### Goodness of fit

The **coefficient of determination** tells you the proportion of variance explained by the model:

$$
r^2 = \frac{\sum_{i=1}^n \left ( \hat{y_i} - \bar{y} \right )^2}
{\sum_{i=1}^n \left ( y_i - \bar{y} \right )^2}
$$


## Regression assumptions
* Data points are **independent** (*i.i.d*)
* The residuals must be normally distributed 
    - always true if $x$ and $y$ follow a **multivariate normal**
* Homoscedasticity (constant variance of the residuals: $s_{\epsilon}$)
* The relationship between $x$ and $y$ is linear
   - valid if $y$ is linear w.r.t some transformation: $\hat{y} = a + b\mathcal{f}(x)$
   - more complicated in the reverse case: $\mathcal{f}(\hat{y}) = a + bx$




## Transformations

<div class="left lt">
* Transformation can be used to linearize non-linear relationships
* It can also improve the quality of the variance of the residuals
* Use caution! The hypothesized relationship will change!

</div>
<div class="right rt">
```{r, echo = FALSE, fig.height = 8}
# Exponential relationship between x and y
set.seed(10)
x = rnorm(400, 10, 1)
y = exp(0.1 + 0.5 * x + rnorm(400, 0, 0.5))
par(mfrow = c(2, 1))
plot(x,y, pch = 16, cex = 0.6)
plot(log(x), log(y), pch = 16, cex = 0.6)

```
</div>

## Regression in R

<div class="left lt">

```{r}
## Data on sparrow wing lengths from Zar (1984)
dat = data.frame(age = c(3:6, 8:12, 14:17), 
        wing_length = c(1.4, 1.5, 2.2, 2.4, 3.1, 3.2, 3.2, 
        				3.9, 4.1, 4.7, 4.5, 5.2, 5.0))
mod = lm(wing_length ~ age, data = dat)
summary(mod)
confint(mod)

```
</div>

<div class="left lt">
```{r echo = FALSE}
plot(wing_length ~ age, data = dat, pch=16, col="blue", bty='n',
     ylab= "Sparrow wing length (cm)", xlab = "Age (days)")
abline(mod)
```
</div>

## Regression in R: diagnostics

```{r fig.width = 10}
par(mfrow=c(1, 2), bty='n')

## scale(residuals) produces **standardized** residuals
qqnorm(scale(residuals(mod)), pch=16)
qqline(scale(residuals(mod)), lty=2)
plot(dat$age, residuals(mod), xlab = "age", ylab = "residuals", pch=16)
abline(h = 0, lty=2)

## also try
## plot(mod)
```


## Regression in R: diagnostics

```{r fig.width = 7}
## also try
par(mfrow = c(2,2), bty='n', mar = c(3, 3, 2,0), mgp = c(2, 0.5, 0))
plot(mod)
```

## Presenting regression output

<div class="left lt">
* **Always**:
  - coefficients and standard errors
  - degrees of freedom or $n$
  - $p$-value, maybe $F$ or $t$ statistic
* Recommended
  - Some metric of goodness of fit (e.g., $R^2$)
  - Visualisation of the relationship
* In supplemental material
  - Diagnostic plots
  - Models tested but not presented

</div>

<div class="right rt">
We found a significant positive relationship between wing length and age ($F_{1,11} = 400$, $p < 0.001$, $R^2 = 0.97$; Table 1). <br/>

```{r echo = FALSE, results = 'asis'}
tab = data.frame(" " = c("", "Intercept      ", "Age"), 
				 estimate = c("Estimate      ", "0.71", "0.27"), 
                 s = c("St. error      ", "0.15", "0.013"), 
				 "95\\% CI" = c("95% CI", "[0.39, 1.03]", "[0.24, 0.30]"))
kt = kable(tab, col.names = c("", "", "", ""),
      caption = "Table 1. Parameter estimates for regression of wing length on age")
kt = row_spec(kt, 1, extra_css = "border-top: solid 2px; border-bottom: solid 1px")
kt = row_spec(kt, 2, extra_css = "padding 20px")
kt
```

</div>

