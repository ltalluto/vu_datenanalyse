---
title: "Correlation and Regression"
author: "Matthew Talluto"
date: "14.02.2023"
output:
  slidy_presentation:
    theme: cerulean
    toc_depth: 2
    css: ../assets/rmd_style.css
  beamer_presentation: default
---


```{r setup, include=FALSE}
## r code goes in to chunks, delimited like this. you can add one with the "Insert" menu
## This setup chunk should be used for defining the document defaults
## here, we set the default figure size, the comment character, and the type
## of graphics to use
knitr::opts_chunk$set(fig.width=5.5, fig.height=5.5, collapse = TRUE, comment = "##", dev="png", error=TRUE)

## this chunk us also a cood place to load any librarys you want
## and to store constants in the R environment
## here I save a set of 8 colours for figures
library(RColorBrewer)
cols = brewer.pal(8, "Set1")
library(mvtnorm)
library(ggplot2)
library(gridExtra)
## These two packages make table display nicer; you can omit them if you don't want
## to use tables
library(knitr)
library(kableExtra)
options(digits = 3)
set.seed(12345)
```


## Covariance

<div class="left lt">

* If two variables $x$ and $y$ are (**independent**), we can say that if $x$ is known, we have no additional information about $y$ (and vice-versa). 
* In other words, the distribution of values of y does not change with respect to x.

$$
pr(y|x) = pr(y)
$$
</div>
<div class="right rt">

We can use the `plot` function to make a bivariate scatterplot in order to visualise this:

```{r fig.cap = "Independent random variables"}
x = rnorm(1000)
y = rnorm(1000) # these variables are independent

plot(x, y, pch=16, bty='n')
```

</div>



## Covariance

<div class="left lt">

* Two variables that **covary** do not have this independence property; the values in $x$ can be used to predict $y$ (with some error), and vice-versa.
* The **sample covariance** ($\mathrm{cov}_{xy}$) looks similar to the equation for the sample variance, but relates to the amount of variation that is shared between $x$ and $y$

$$
\mathrm{cov}_{xy} = \frac{\sum_{i=1}^n \left (x_i - \bar{x} \right) \left(y_i - \bar{y} \right)}{n-1}
$$

```{r echo = FALSE}
sig1 = matrix(c(1, 0.9, 0.9, 1), ncol=2)
sig2 = matrix(c(1, -0.4, -0.4, 1), ncol=2)
sig3 = matrix(c(1, 0, 0, 1), ncol=2)
xx1 = rmvnorm(1000, sigma = sig1)
x1 = xx1[,1]; y1 = xx1[,2]
xx2 = rmvnorm(1000, sigma = sig2)
x2 = xx2[,1]; y2 = xx2[,2]
xx3 = rmvnorm(1000, sigma = sig3)
x3 = xx3[,1]; y3 = xx3[,2]
p1 = ggplot(data.frame(x=x1, y = y1), aes(x=x, y=y)) + geom_point() +
	xlab(expression(x[1])) + ylab(expression(y[1])) +
	ggtitle("Strong Positive Covariance") + theme_minimal()
p2 = ggplot(data.frame(x=x2, y = y2), aes(x=x, y=y)) + geom_point() +
	xlab(expression(x[2])) + ylab(expression(y[2])) +
	ggtitle("Weak Negative Covariance") + theme_minimal()
p3 = ggplot(data.frame(x=x3, y = y3), aes(x=x, y=y)) + geom_point() +
	xlab(expression(x[3])) + ylab(expression(y[3])) +
	ggtitle("Covariance = 0") + theme_minimal()

```

```{r}
# in R:
c(  top =    cov(x1, y1),
	middle = cov(x2, y2),
	bottom = cov(x3, y3))
```

</div>
<div class="right rt">

```{r echo = FALSE, warning = FALSE, fig.height = 10}
grid.arrange(p1, p2, p3, ncol=1)
```

</div>



## Correlation

<div class="left lt">

* Like with the mean and standard deviation, we can **rescale** the covariance to make it easier to compare different datasets.
* Scale-independent covariance is called **correlation**; for many datasets we use the **Pearson correlation coefficient** $\rho_{xy}$.
* Ranges from $-1$ to $1$. Zero means zero covariance. $-1$ and $1$ indicates that $x$ and $y$ predict each other perfectly!

$$
\begin{aligned}
\mathrm{cov}_{xy} & = \frac{\sum_{i=1}^n \left (x_i - \bar{x} \right) \left(y_i - \bar{y} \right)}{n-1} \\
r_{xy} & = \frac{\mathrm{cov}_{xy}}{s_x s_y}
\end{aligned}
$$

```{r}
# in R:
c(  top =    cor(x1, y1),
	middle = cor(x2, y2),
	bottom = cor(x3, y3))
```


</div>

<div class="right rt">

```{r echo = FALSE, warning = FALSE, fig.height = 10}
grid.arrange(
	p1 + annotate("label", x=-2.5, y=2, label=paste("r =", round(cor(x1, y1), 1))), 
	p2 + annotate("label", x=-2.5, y=2, label=paste("r =", round(cor(x2, y2), 1))), 
	p3 + annotate("label", x=-2.5, y=2, label=paste("r =", round(cor(x3, y3), 1))), ncol=1)
```


</div>

## Correlation significance testing

$H_0$: $r = 0$

$H_A$: two sided ($\rho \ne 0$) or one-sided ($\rho > 0$ or $\rho < 0$)

$r$ has a standard error:


$$
s_{r} = \sqrt{\frac{1-r^2}{n-2}}
$$
We can then compute a $t$-statistic:

$$
t = \frac{r}{s}
$$

The probability that $t > \alpha$ (i.e., use the **CDF** of the t distribution) is the p-value.

## Correlation test in R

<div class="left lt">

$$
\begin{aligned}
s_{r} &= \sqrt{\frac{1-r^2}{n-2}} \\
t &= \frac{r}{s}
\end{aligned}
$$

```{r fig.show='hide'}
data(penguins, package = "palmerpenguins")
penguins = as.data.frame(penguins)
penguins = penguins[complete.cases(penguins),] # remove NAs

penguins_ade = subset(penguins, species == "Adelie")
n = nrow(penguins_ade)
(r = cor(penguins_ade$body_mass_g, penguins_ade$flipper_length_mm))
(s_r = sqrt((1-r^2)/(n-2)))
(t_val = r/s_r)
(2 * pt(t_val, n-2, lower.tail = FALSE)) ## two-sided test

```

</div>
<div class="right rt">
### Visualisation

The classic way to visualise is a **scatterplot**

```{r fig.height=4, fig.width=4}
plot(flipper_length_mm ~ body_mass_g, data = penguins_ade, pch=16, 
     xlab = "Adelie Penguin Body Mass (g)", ylab = "Flipper Length (mm)", bty='n')
```

</div>

## Correlation test in R


<div class="left lt">

$$
\begin{aligned}
s_{r} &= \sqrt{\frac{1-r^2}{n-2}} \\
t &= \frac{r}{s}
\end{aligned}
$$

```{r fig.show='hide'}
penguins_ade = subset(penguins, species == "Adelie")
n = nrow(penguins_ade)
(r = cor(penguins_ade$body_mass_g, penguins_ade$flipper_length_mm))
(s_r = sqrt((1-r^2)/(n-2)))
(t_val = r/s_r)
(2 * pt(t_val, n-2, lower.tail = FALSE)) ## two-sided test
```

</div>

<div class="right rt">
```{r}
# Equivalent built-in test
with(penguins_ade, cor.test(body_mass_g, flipper_length_mm, alternative = "two.sided"))
```

</div>


## Correlation test: assumptions

* Data must be at least interval scale
  - ordinal data: Spearman rank correlation (avialable in `cor.test` and `cor`)
  - nominal data: Association test (`prop.test`) or $\chi^2$ test (`chisq.test`)
* Population is distributed *bivariate normal* (or $n$ is sufficiently large)


## Correlation pitfalls
* Test is misleading if relationship is nonlinear

![](img/cor_example.png){width=80%}


## Correlation pitfalls

<div class="left lt">

Heterogeneity of subgroups


```{r echo = FALSE}
csg = read.csv("data/cor_subgroup.txt")
csg = csg[order(csg$sex, csg$x),]
mod = coef(lm(y~x, data = csg))
mod2 = lm(y~x*sex, data = csg)
segs = data.frame(xs = csg$x[c(1,10)], xe = csg$x[c(9, nrow(csg))], 
				   ys = predict(mod2)[c(1,10)], 
				   ye = predict(mod2)[c(9, nrow(csg))], sex = c('f', 'm'))
ggplot(csg, aes(x=x, y=y, col=sex)) + geom_point() + ylab("Fish Weight") + 
	xlab("Fish Length") + theme_minimal() + 
	geom_abline(aes(slope=mod[2], intercept = mod[1]), linetype=2) +
	geom_segment(data = segs, aes(x=xs, y=ys, xend=xe, yend=ye)) + 
	theme(axis.ticks=element_blank(), axis.text = element_blank())
```

</div>
<div class="right rt">

</div>

## Spearman correlation

<div class="left lt">

* Used when Pearson correlation assumptions are violated
	- non-normal data
	- non-linear (but monotonic) relationships
	- ordinal data
	
```{r, echo = FALSE}
set.seed(123)
n = 150
x = rlnorm(n)
y = exp(-1.5*x) + rgamma(n, 0.75, 0.75)
plot(x, y, pch=16, bty='n')
```
</div>
<div class="right rt">


```{r}
cor.test(x, y)
```

</div>

## Spearman correlation
<div class="left lt">

* Used when Pearson correlation assumptions are violated
	- non-normal data
	- non-linear (but monotonic) relationships
	- ordinal data

```{r, echo = FALSE}
plot(rank(x), rank(y), pch = 16, bty='n')
```

* The math is simple: rank transform x and y, then compute Pearson correlation.

</div>
<div class="right rt">

```{r}
cor.test(x, y)
cor.test(x, y, method = 'spearman')
```

</div>

## Tests of association: 2 × 1 Tables


<div class="left lt">
### Mendel's peas

* Take homozygous violet (dominant) and white (recessive plants)
* $F_1$ plants are 100% violet
* Theory says that $F_2$ plants should be 75% violet, 25% white

Observation:

```{r echo = FALSE, results = 'asis'}
tab = data.frame("colour" = c("violet", "white"), F2 = c(705, 224))
kable(tab)
```


</div>

<div class="right rt">
![](img/pea.jpg){width=75%}

<p style="font-size:x-small;">[source](https://opentextbc.ca/biology/chapter/8-1-mendels-experiments/)</p>


</div>





## Tests of association: 2 × 1 Tables

<div class="left lt">
### Mendel's peas

* Take homozygous violet (dominant) and white (recessive plants)
* $F_1$ plants are 100% violet
* Theory says that $F_2$ plants should be 75% violet, 25% white

Observation:

```{r echo = FALSE, results = 'asis'}
tab = data.frame("colour" = c("violet", "white"), F2 = c(705, 224))
kable(tab)
```

</div>

<div class="right rt">

$H_0$: Inheritance is Mendelian (violet:white = 3:1)

$H_A$: Inheritance is *not* exactly Mendelian


```{r}
# Test of proportions against a null hypothesis
# For small sample sizes, use binom.test
counts = matrix(c(705, 224), ncol = 2)
prop.test(counts, p = 0.75, alternative = "two.sided")
```

</div>



## Tests of association: n × n Tables

<div class="left lt">

### Nesting holes of black-backed woodpeckers.

```{r}
woodpecker = read.csv("data/woodpecker.csv")
head(woodpecker)

table(woodpecker)
```

We want to test for an *association* between the two variables (forest type and nest tree)

</div>

<div class="right rt">

![](img/woodpecker.jpg){width=75px}

$H_0$: Nesting tree is not associated with forest type

$H_A$: Nest tree is associated with forest type

</div>


## Chi-squared test

<div class="left lt">

### Nesting holes of black-backed woodpeckers.

```{r}
table(woodpecker)

table(woodpecker)/rowSums(table(woodpecker))

```

We want to test for an *association* between the two variables (forest type and nest tree)

$H_0$: Nesting tree is not associated with forest type
$H_A$: Nest tree is associated with forest type


</div>


## Chi-squared test

<div class="left lt">

### Nesting holes of black-backed woodpeckers.

```{r}
table(woodpecker)

table(woodpecker)/rowSums(table(woodpecker))

```

We want to test for an *association* between the two variables (forest type and nest tree)

$H_0$: Nesting tree is not associated with forest type
$H_A$: Nest tree is associated with forest type


</div>


<div class="right rt">

```{r}
# for 2x2 tables with small sample sizes: Fisher's exact test
# fisher.test()
with(woodpecker, chisq.test(forest_type, nest_tree))

```

</div>




## Visualisation: Ordinal Data

<div class="left lt">

Scatterplots become less useful.

```{r, fig.show='hide'}
birddiv = read.csv("data/birddiv.csv")
bird_plot = ggplot(birddiv, aes(x=forest_frag, 
				y = richness, colour = bird_type)) + 
				geom_point() + theme_minimal()
head(birddiv)
```

</div>

<div class="right rt">
```{r, echo = FALSE}
bird_plot
```
</div>



## Visualisation: Ordinal Data

<div class="left lt">

Adding jitter can sometimes improve things

```{r, fig.show='hide'}
bird_plot = ggplot(birddiv, aes(x=forest_frag, 
				y = richness, colour = bird_type)) + 
				geom_jitter() + theme_minimal()
```

</div>

<div class="right rt">
```{r, echo = FALSE}
bird_plot
```
</div>





## Visualisation: Ordinal Data

<div class="left lt">

Better solution: boxplots

```{r, fig.show='hide'}
bird_plot = ggplot(birddiv, aes(x=as.factor(forest_frag), 
				y = richness, fill = bird_type)) + 
				geom_boxplot() + theme_minimal()
bird_plot = bird_plot + xlab("Forest Fragmentation")
```

</div>

<div class="right rt">
```{r, echo = FALSE}
bird_plot
```
</div>




## Visualisation: Categorical Data

You see this a lot. When should you do it? **NEVER**

```{r, echo = FALSE}
tab_burned = table(woodpecker[woodpecker$forest_type == "burned", "nest_tree"])
tab_burned = as.data.frame(tab_burned)
colnames(tab_burned) = c("nest_tree", "count")

tab_unburned = table(woodpecker[woodpecker$forest_type == "unburned", "nest_tree"])
tab_unburned = as.data.frame(tab_unburned)
colnames(tab_unburned) = c("nest_tree", "count")

p1 = ggplot(tab_burned, aes(x = "", y = count, fill = nest_tree)) +
  geom_col() + xlab("") + ylab("") + ggtitle("Burned") + theme_minimal() + 
  coord_polar(theta = "y") + theme(axis.text.x=element_blank(),
      axis.ticks.x=element_blank(),
      axis.text.y=element_blank(),
      axis.ticks.y=element_blank())
p2 = ggplot(tab_unburned, aes(x = "", y = count, fill = nest_tree)) +
  geom_col() + xlab("") + ylab("") + ggtitle("Unburned") + theme_minimal() + 
  coord_polar(theta = "y") + theme(axis.text.x=element_blank(),
      axis.ticks.x=element_blank(),
      axis.text.y=element_blank(),
      axis.ticks.y=element_blank())
library(gridExtra)
grid.arrange(p1, p2, ncol=2)
```


## Visualisation: Categorical Data

This is almost as bad, and sadly much more common

```{r, echo = FALSE}

hsize = 4
tab_burned$x = tab_unburned$x = hsize

p3 = ggplot(tab_burned, aes(x = x, y = count, fill = nest_tree)) +
  geom_col() + xlab("") + ylab("") + ggtitle("Burned") + theme_minimal() + 
  coord_polar(theta = "y") + theme(axis.text.x=element_blank(),
      axis.ticks.x=element_blank(),
      axis.text.y=element_blank(),
      axis.ticks.y=element_blank()) + xlim(c(0.2, hsize + 0.5))
p4 = ggplot(tab_unburned, aes(x = x, y = count, fill = nest_tree)) +
  geom_col() + xlab("") + ylab("") + ggtitle("Unburned") + theme_minimal() + 
  coord_polar(theta = "y") + theme(axis.text.x=element_blank(),
      axis.ticks.x=element_blank(),
      axis.text.y=element_blank(),
      axis.ticks.y=element_blank()) + xlim(c(0.2, hsize + 0.5))
grid.arrange(p3, p4, ncol=2)
```


## Visualisation: Categorical Data

<div class="left lt">

Barplots, or proportional bars for counts within categories

```{r, fig.show='hide'}
table(woodpecker)

woodp_plot = ggplot(woodpecker, aes(x = nest_tree,
				fill = forest_type)) + theme_minimal()
woodp_plot = woodp_plot + geom_bar(width = 0.5)
woodp_plot
```

Stacked bars are "unfair" — easiest to compare the "rooted" class (unburned).

</div>

<div class="right rt">
```{r, echo = FALSE}
woodp_plot
```
</div>

## Visualisation: Categorical Data

<div class="left lt">

Barplots, or proportional bars for counts within categories

```{r, fig.show='hide'}
table(woodpecker)

woodp_plot = ggplot(woodpecker, aes(x = nest_tree,
				fill = forest_type))
woodp_plot = woodp_plot + geom_bar(width = 0.5, 
					position=position_dodge())
woodp_plot = woodp_plot + xlab("Nest Tree Type") +
	theme_minimal() + labs(fill = "Forest Type")
woodp_plot
```

</div>

<div class="right rt">
```{r, echo = FALSE}
woodp_plot
```
</div>




## The linear model

* Correlation assumes nothing about the *causal* nature of the relationship between x and y
* Often, we have a hypothesis that a variable is in some way **functionally dependent** on another

* A **simple linear regression** describes/tests the relationship between
   - the **independent variable** (x, aka predictor or input variable), and
   - the **dependent variable** (y, aka response or outcome)


$$
\begin{aligned}
\mathbb{E}(y|x) & = \hat{y} = \alpha + \beta x \\
 & \approx a + bx \\ \\
\end{aligned}
$$

$y$ is not perfectly predicted by $x$, so we must include an error (residual) term:

$$
\begin{aligned}
y_i & = \hat{y_i} + \epsilon_i  \\ 
& = a + bx_i + \epsilon_i\\ \\
\epsilon & \sim \mathcal{N}(0, s_{\epsilon})
\end{aligned}
$$

## Method of least squares

$$
\begin{aligned}
\hat{y_i} & = a + b x_i \\
y_i & = \hat{y_i} + \epsilon_i  \\ 
\epsilon & \sim \mathcal{N}(0, s_{\epsilon})
\end{aligned}
$$

We want to solve these equations for $a$ and $b$

The "best" $a$ and $b$ are the ones that draw a line that is closest to the most points (i.e., that minimizes $s_{\epsilon}$)

```{r echo = FALSE, fig.width=12}
pars = coef(lm(flipper_length_mm ~ body_mass_g, data = penguins_ade))

par(mfrow = c(1, 3))
plot(penguins_ade$body_mass_g, penguins_ade$flipper_length_mm, pch=16, 
     xlab = "Adelie body mass (g)", 
     ylab = "Adelie flipper length (mm)", bty='n', main = expression(best~line))
abline(a = pars[1], b = pars[2], col='blue', lwd=2)

plot(penguins_ade$body_mass_g, penguins_ade$flipper_length_mm, pch=16, 
     xlab = "Adelie body mass (g)", 
     ylab = "Adelie flipper length (mm)", bty='n', 
     main = expression(bias:~bar(epsilon) != 0))
abline(a = pars[1] + 15, b = pars[2], col='blue', lwd=2)

plot(penguins_ade$body_mass_g, penguins_ade$flipper_length_mm, pch=16, 
     xlab = "Adelie body mass (g)", 
     ylab = "Adelie flipper length (mm)", bty='n', 
     main = expression(precision:~s[epsilon]~too~large))
abline(a =mean(penguins_ade$flipper_length_mm), b = 0, col='blue', lwd=2)

```


## Method of least squares

$$
s_{\epsilon} = \sqrt{\frac{\sum_{i=1}^n \left (y_i -\hat{y_i}\right )^2}{n-2}}
$$

* $n-2$ is constant w.r.t. $a$ and $b$
* Simpler to compute the **Error** (aka **residual**) **Sum of Squares**:

$$
\begin{aligned}
\mathrm{ESS} & = \sum_{i=1}^n \left (y_i -\hat{y_i}\right )^2 \\
& = \sum_{i=1}^n \left (y_i - a - bx_i \right )^2
\end{aligned}
$$

## Ordinary least squares estimation

Solving for the minimum ESS yields:

$$
\begin{aligned}
b & = \frac{\mathrm{cov_{xy}}}{s^2_x} \\ \\
  & = r_{xy}\frac{s_y}{s_x}\\ \\
a & = \bar{y} - b\bar{x}
\end{aligned}
$$

The parameters have standard errors:

$$
s_a = s_{\hat{y}}\sqrt{\frac{1}{n} + \frac{\bar{x}^2}{\sum{(x-\bar{x}}^2)}}
$$

$$
s_b = \frac{s_{\hat{y}}}{\sqrt{\sum{(x-\bar{x}}^2)}}
$$

## Significance tests

$\mathbf{H_0}$: The slope $\beta$ = 0 (i.e., no variation in $y$ is explained by variation in $x$)

The ratio of variance explained to residual variance follows an $F$ distribution



\begin{aligned}
F & = \frac{MS_{model}}{MS_{err}} \\

MS_{model} & = \sum_{i=1}^n \left ( \hat{y}_i - \bar{y}\right)^2 \\
MS_{err} & = \frac{\sum_{i=1}^n \left ( y_i - \hat{y}_i \right)^2}{n-1}
\end{aligned}

### Goodness of fit

The **coefficient of determination** tells you the proportion of variance explained by the model:

$$
r^2 = \frac{\sum_{i=1}^n \left ( \hat{y_i} - \bar{y} \right )^2}
{\sum_{i=1}^n \left ( y_i - \bar{y} \right )^2}
$$


## Regression assumptions
* Data points are **independent** (*i.i.d*)
* The residuals must be normally distributed 
    - always true if $x$ and $y$ follow a **multivariate normal**
* Homoscedasticity (constant variance of the residuals: $s_{\epsilon}$)
* The relationship between $x$ and $y$ is linear
   - valid if $y$ is linear w.r.t some transformation: $\hat{y} = a + b\mathcal{f}(x)$
   - more complicated in the reverse case: $\mathcal{f}(\hat{y}) = a + bx$




## Transformations

<div class="left lt">
* Transformation can be used to linearize non-linear relationships
* It can also improve the quality of the variance of the residuals
* Use caution! The hypothesized relationship will change!

</div>
<div class="right rt">
```{r, echo = FALSE, fig.height = 8}
# Exponential relationship between x and y
set.seed(10)
x = rnorm(400, 10, 1)
y = exp(0.1 + 0.5 * x + rnorm(400, 0, 0.5))
par(mfrow = c(2, 1))
plot(x,y, pch = 16, cex = 0.6)
plot(log(x), log(y), pch = 16, cex = 0.6)

```
</div>

## Regression in R

<div class="left lt">

```{r}
## Data on sparrow wing lengths from Zar (1984)
dat = data.frame(age = c(3:6, 8:12, 14:17), 
        wing_length = c(1.4, 1.5, 2.2, 2.4, 3.1, 3.2, 3.2, 
        				3.9, 4.1, 4.7, 4.5, 5.2, 5.0))
mod = lm(wing_length ~ age, data = dat)
summary(mod)
confint(mod)

```
</div>

<div class="left lt">
```{r echo = FALSE}
plot(wing_length ~ age, data = dat, pch=16, col="blue", bty='n',
     ylab= "Sparrow wing length (cm)", xlab = "Age (days)")
abline(mod)
```
</div>

## Regression in R: diagnostics

```{r fig.width = 10}
par(mfrow=c(1, 2), bty='n')

## scale(residuals) produces **standardized** residuals
qqnorm(scale(residuals(mod)), pch=16)
qqline(scale(residuals(mod)), lty=2)
plot(dat$age, residuals(mod), xlab = "age", ylab = "residuals", pch=16)
abline(h = 0, lty=2)

## also try
## plot(mod)
```


## Regression in R: diagnostics

```{r fig.width = 7}
## also try
par(mfrow = c(2,2), bty='n', mar = c(3, 3, 2,0), mgp = c(2, 0.5, 0))
plot(mod)
```

## Presenting regression output

<div class="left lt">
* **Always**:
  - coefficients and standard errors
  - degrees of freedom or $n$
  - $p$-value, maybe $F$ or $t$ statistic
* Recommended
  - Some metric of goodness of fit (e.g., $R^2$)
  - Visualisation of the relationship
* In supplemental material
  - Diagnostic plots
  - Models tested but not presented

</div>

<div class="right rt">
We found a significant positive relationship between wing length and age ($F_{1,11} = 400$, $p < 0.001$, $R^2 = 0.97$; Table 1). <br/>

```{r echo = FALSE, results = 'asis'}
tab = data.frame(" " = c("", "Intercept      ", "Age"), 
				 estimate = c("Estimate      ", "0.71", "0.27"), 
                 s = c("St. error      ", "0.15", "0.013"), 
				 "95\\% CI" = c("95% CI", "[0.39, 1.03]", "[0.24, 0.30]"))
kt = kable(tab, col.names = c("", "", "", ""),
      caption = "Table 1. Parameter estimates for regression of wing length on age")
kt = row_spec(kt, 1, extra_css = "border-top: solid 2px; border-bottom: solid 1px")
kt = row_spec(kt, 2, extra_css = "padding 20px")
kt
```

</div>

