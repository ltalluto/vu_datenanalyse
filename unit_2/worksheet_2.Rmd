---
title: "Worksheet 2"
author: "Matthew Talluto & Gabriel Singer"
date: "13.02.2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, error=TRUE)
```

## Environmental values and environmental education

Load the environmental values dataset (Pinder et al, 2020, https://doi.org/10.5061/dryad.wdbrv15kn).

```{r}
env = read.csv("data/env_values.csv")
```

University students in Australia were asked various questions about their environmental values, desires for conservation careers, and educational background. We will examine two variables to address the hypothesis that students from families that value the environment rate more highly the quality of their education about environmental values.

The data has two variables:

* `env_education` contains responses to the question, "Thinking back to your schooling overall, how adequate was your education about environmental problems?", ranging from 0 (extremely inadequate) to 5 (extremely adequate).
* `value_of_env_protection` has responses for, "How important is protecting the environment in your family?", ranging from 0 (not important) to 7 (very important)... note that one student indicated -1 ;-)

1. Produce a histogram for each variable and a scatterplot for both variables. How do you interpret these plots? 
2. Which type of correlation is most appropriate for these two variables?
3. Use the function `cor.test` on `env_education` and `value_of_env_protection`. Is there a significant correlation? What direction is it? What is the estimate of the correlation?

**Hint**: Choose the type of correlation using `method` in `cor.test`. There are missing values in the data, you will need to deal with these. 


## Hydropsyche width-mass relationship

The following dataset collects head capsule widths and body masses from Hydropsyche (a genus of caddisflies) in the Danube in Austria. Note that this data file is tab delimited, not comma delimited, so you will need `read.table`.

```{r}
hydrop = read.table("data/Hydropsyche.txt", header = TRUE)
```

1. As before, do some data exploration; plot the variables against each other, examine histograms, and possibly boxplots as well. How do you interpret these plots? Can you say anything about the relationship between these variables?
2. Use `lm` to fit a simple linear model (one variable, no transformations), using body mass as the response (y) variable, and width as the predictor. Is the regression significant? Report the statistics as shown in the lecture.

```{r eval = FALSE}
# use the formula syntax within lm to describe the model you want to fit
mod1 = lm(weight ~ width, data = hydrop)

# use the summary function to get information from your model
# how do you interpret the output?
summary(mod1)
```


3. Produce some diagnostic plots of your model (see slide 18: Regression in R: diagnostics, or use `plot(mod)`). Is this an adequate model? Does it meet the assumptions of the linear model?
4. Based on what you learned in 3, produce a new model, this time using a transformation of $x$ and/or $y$. Repeat 2 and 3 for the new model. Is the new model better? What does the new model imply about the relationship between mass and size? Is it testing the same hypothesis as the previous model?

**Bonus**: Produce a scatterplot of width (x-axis) and mass (y-axis), on the original scale with no transformations. Add the regression curve from question 3 to your plot.

```{r eval = FALSE}
# to get you started
ggplot(hydrop, aes(x = ?, y = ?)) + geom_point() + 
	geom_smooth(method = "lm", formula = ?)
```



## Solid particulate matter in lakes

Load the LakeSPM dataset (Lindström, M., Håkanson, L., Abrahamsson, O., Johansson, H. (1999) An empirical model for prediction of lake water suspended particulate matter. Ecological Modelling 121, 185–198).

```{r}
lakes = read.table("data/LakeSPM.txt", header=TRUE)
```

The study has a set of lakes with SPM=solid particulate matter as the main response of interest. High SPM is a water quality issue, it can be generated through excessive phytoplankton growth or resuspension of sediment in shallow lakes or terrigeneous input into smaller lakes. For water quality prediction a model to predict SPM from easily accessible lake data should be generated. The available predictors include

* Measures of lake size and morphology that could influence sensitivity to terrestrial loading and resuspension of lake sediment: area, mean depth Dm, maximum depth Dmax, dynamic ratio DR (=sqrt(area)/Dm), volume development form factor Vd (3*Dm/Dmax)
* Measures of water renewal: discharge into lake Q, turnover time T in years
* Proxies for productivity: pH (buffering capacity), total phosphorous TP

1. Formulate a set of hypotheses testing effects of productivity and lake depth on spm. Choose variables wisely, then run a formal hypothesis test using MLR.
A suggestion is:
H1: Productive lakes support phytoplankton growth, thus higher spm.
H2: Shallow lakes allow resuspension, thus higher spm.
Use variables TP and Dm, consider log-transformation (also of spm) to improve linearity.

2. These hypotheses are lame ;-) We know that nutrients influence productivity since decades. For water quality forecasting a model to predict spm is needed. Start an explorative approach with the aim to design a *good* model for spm. Explore the data for potential relationships with the response SPM. Assess distributions and chance for collinearity by graphical means (e.g. using `pairs` or `plot(data)`). Consider appropriate transformation to improve linearity of relationships.
3. With `vif` from the `car`-package you can assess collinearity. Identify redundant variables using VIF and drop them from the dataset.

```{r eval=FALSE}
lakes = lakes[,-c(1:2)]
plot(lakes) # skewness issues!
boxplot(lakes)
boxplot(scale(lakes))
lakes[, -c(5,10)] = log(lakes[, -c(5,10)])
plot(lakes)

# . makes a model with all predictors available
mlr.full = lm(spm ~ ., data = lakes) 
summary(mlr.full)
library(car)
vif(mlr.full) #computes VIF
summary(lm(area ~ . - spm, data = lakes)) #r2=1!!

# try a few reductions
# the .- syntax fits all variables *except* the ones with a -
vif(lm(spm ~ . - area, data = lakes))
vif(lm(spm ~ . - area - Dm, data = lakes))
vif(lm(spm ~ . - area - Dm - Dmax, data = lakes))

names(lakes)

```

4. Build up a full model using all the non-redundant predictors
```{r eval=FALSE}
# drop predictors that we killed with VIF
lakes = lakes[, -c(2:4)]
mf = lm(spm ~ ., data = lakes)
summary(mf)
```

5. Use `step` to identify a *good* model based on the AIC. Be sure to check diagnostics of the final model you get. Can you make conclusions about significance?

```{r eval=FALSE}
step(mf, direction="both")

summary(m3<-lm(spm~pH + TP + DR + Vd,data=lakes))
```


### Bonus

Try a leave-one-out cross validation approach to test the model's predictive ability. The procedure should look like this:

1. Take your best/favourite model from above, and also a second model for comparison. 
2. Drop the first data point from the lakes data frame.
3. Fit both models on the new dataset with n-1 rows.
4. Use the models to predict $y$ for the left out case.
5. Compute the squared error for the left out case for both models: $sqerr = (y_{predicted} - y_{measured})^2$
6. Use a loop to repeat steps 2—5 $n$ times, each time dropping a different data point. Save the error each time.
7. Compute the root mean square error (RMSE) for each model: $RMSE = \sqrt{\mathrm{mean}(sqerr)} $
8. Plot the observed vs the predicted values for each left-out case

Some syntax to get you started
```{r eval = FALSE}
n = nrow(lakes)
# create empty vectors to store predicted cases
preds_1 = numeric(n)
preds_2 = numeric(n)
# loop over every data point
for(i in 1:n) {
	# drop data point i
	lakes_sm = lakes[-i, ]
	
	# fit the models
	m1 = ??
		
	# use the predict() function to get a model prediction
	# see ?predict.lm for clues
	preds_1[i] = predict(m1, ??)
}
```


