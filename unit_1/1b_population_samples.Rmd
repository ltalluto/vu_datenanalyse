---
title: "Populations & Samples"
author: "Matthew Talluto"
date: "13.02.2023"
output:
  slidy_presentation:
    theme: cerulean
    toc_depth: 2
    css: ../assets/rmd_style.css
  beamer_presentation: default
---


```{r setup, include=FALSE}
# knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.width=6.5, fig.height=6.5, collapse = TRUE, comment = "##", dev="png", error=TRUE, message = FALSE)
#knitr::opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)


library(knitr)
library(kableExtra)
options(digits = 3)
set.seed(12345)
data(penguins, package = "palmerpenguins")
penguins = as.data.frame(penguins)
penguins = penguins[complete.cases(penguins), ]

```


















## Probability distributions

<div class="left lt">

**Thought experiment**

You and 99 of your closest friends gather on the centre line of a 100-m football pitch. We define the centre line as 0m, the western boundary -50 m, and the eastern boundary as +50 m.


TODO: Make this an actual lesson they do. Give them the draw_pitch() and draw_players() functions, maybe also a wrapper to animate (or maybe too much). Also give them the simulation code, they just need to run it and make a histogram in the end, maybe change the number of players or the number of time steps. draw_players should be able to flexibly draw them no matter how many
</div>

<div class="right rt">

```{r echo=FALSE, fig.height=4}
library("plotrix")
draw_pitch = function() {
	grn = "#39A83B"
	xl = c(-50, 50)
	yl = c(0, 66)
	par(mar = c(2,0,0,0))
	plot(0,0, xlim=xl+c(-5, 5), ylim = yl+c(-5, 5), type='n', axes = FALSE, ylab = '', xlab = '', bty='n', asp=1)
	axis(1, seq(xl[1], xl[2], 25))
	rect(xl[1]-5, yl[1]-5, xl[2]+5, yl[2]+5, col = grn, border = 'white')
	rect(xl[1], yl[1], xl[2], yl[2], border = 'white')
	
	draw.arc(rep(xl, each=2), rep(yl, 2), 0.5, deg1=c(0, 270, 90, 180), deg2 = c(90, 360, 180, 270), col = 'white')
	
	
	draw.circle(xl[1]+11, median(yl), 9.15, border = 'white')
	draw.circle(xl[2]-11, median(yl), 9.15, border = 'white')
	draw.circle(median(xl), median(yl), 9.15, border = 'white')
	lines(c(0,0), yl, col='white')
	# penalty boxes
	pby = median(yl) + c(-1, 1)*(40.3/2)
	rect(xl[1], pby[1], xl[1]+16.5, pby[2], col = grn, border = 'white')
	rect(xl[2]-16.5, pby[1], xl[2], pby[2], col = grn, border = 'white')
	# goal boxes
	gby = median(yl) + c(-10, 10)
	rect(xl[1], gby[1], xl[1]+5.5, gby[2], col = grn, border = 'white')
	rect(xl[2]-5.5, gby[1], xl[2], gby[2], col = grn, border = 'white')
}
draw_players = function(px, py) {
	points(px, py, pch = 16, col="black", cex=0.5)
}
draw_pitch()
ply = seq(1, 65, length.out = 100)
plx = rep(0, 100)
draw_players(plx, ply)

```

```{r, echo = FALSE, warning = FALSE}
ggplot(data.frame(plx=plx), aes(x=plx)) + geom_histogram(bins=40) + xlab("Player positions") + xlim(-50, 50)
```

</div>





## Probability distributions
<div class="left lt">

**Thought experiment**

You and 99 of your closest friends gather on the centre line of a 100-m football pitch. We define the centre line as 0m, the western boundary -50 m, and the eastern boundary as +50 m.

Each person flips a fair coin. If the coin is heads, they take a step east (add 0.5 m to their location), if its tails, they take a step west (subtract 0.5 m from their location). 

**Question**: What is the long-run distribution of positions on the field?


</div>

<div class="right rt">

```{r echo=FALSE, fig.height=4}
draw_pitch()
plx = plx + sample(c(-0.5, 0.5), length(plx), replace = TRUE)
draw_players(plx, ply)

```

```{r, echo = FALSE, warning = FALSE}
ggplot(data.frame(plx=plx), aes(x=plx)) + geom_histogram(bins=40) + xlab("Player positions") + xlim(-50, 50)
```

</div>





## Probability distributions
<div class="left lt">

**Thought experiment**

You and 99 of your closest friends gather on the centre line of a 100-m football pitch. We define the centre line as 0m, the western boundary -50 m, and the eastern boundary as +50 m.

Each person flips a fair coin. If the coin is heads, they take a step east (add 0.5 m to their location), if its tails, they take a step west (subtract 0.5 m from their location). 

**Question**: What is the long-run distribution of positions on the field?

t = 10

</div>

<div class="right rt">

```{r echo=FALSE, fig.height=4}
draw_pitch()
for(i in 1:9)
	plx = plx + sample(c(-0.5, 0.5), length(plx), replace = TRUE)
draw_players(plx, ply)

```

```{r, echo = FALSE, warning = FALSE}
ggplot(data.frame(plx=plx), aes(x=plx)) + geom_histogram(bins=40) + xlab("Player positions") + xlim(-50, 50)
```

</div>





## Probability distributions
<div class="left lt">

**Thought experiment**

You and 99 of your closest friends gather on the centre line of a 100-m football pitch. We define the centre line as 0m, the western boundary -50 m, and the eastern boundary as +50 m.

Each person flips a fair coin. If the coin is heads, they take a step east (add 0.5 m to their location), if its tails, they take a step west (subtract 0.5 m from their location). 

**Question**: What is the long-run distribution of positions on the field?

t = 50

</div>

<div class="right rt">

```{r echo=FALSE, fig.height=4}
draw_pitch()
for(i in 1:40)
	plx = plx + sample(c(-0.5, 0.5), length(plx), replace = TRUE)
draw_players(plx, ply)

```

```{r, echo = FALSE, warning = FALSE}
ggplot(data.frame(plx=plx), aes(x=plx)) + geom_histogram(bins=40) + xlab("Player positions") + xlim(-50, 50)
```

</div>




## Probability distributions
<div class="left lt">

**Thought experiment**

You and 99 of your closest friends gather on the centre line of a 100-m football pitch. We define the centre line as 0m, the western boundary -50 m, and the eastern boundary as +50 m.

Each person flips a fair coin. If the coin is heads, they take a step east (add 0.5 m to their location), if its tails, they take a step west (subtract 0.5 m from their location). 

**Question**: What is the long-run distribution of positions on the field?

t = 500

</div>

<div class="right rt">

```{r echo=FALSE, fig.height=4}
draw_pitch()
for(i in 1:450)
	plx = plx + sample(c(-0.5, 0.5), length(plx), replace = TRUE)
draw_players(plx, ply)

```

```{r, echo = FALSE, warning = FALSE}
ggplot(data.frame(plx=plx), aes(x=plx)) + geom_histogram(bins=40) + xlab("Player positions") + xlim(-50, 50)
```

</div>






## Probability distributions

We can simulate this experiment in R. Here is code for doing it for one person:

```{r}
position = 0
for(i in 1:100) {
  coin_flip = sample(c("heads", "tails"), 1)
  if(coin_flip == "heads") {
    position = position + 0.5
  } else {
    position = position - 0.5
  }
}
position
```



## Probability distributions
<div class="left lt">

And for a larger sample (2500) friends, doing more coin flips (500 each), with more concise code.

```{r include=FALSE}
set.seed(123)
```

```{r, results=FALSE}
# repeat the initial position 2500 times, one per friend
n_friends = 2500
positions = rep(0, n_friends)

# repeat the experiment 500 times
time_steps = 1:500
for(i in time_steps) {
	# flip a coin for each friend
	# returns one heads or tails for each n_friends
	coin_flips = sample(c("heads", "tails"), size = n_friends, replace = TRUE)
	# compute the steps: +0.5 if the flip is heads, -0.5 if tails
	moves = ifelse(coin_flips == "heads", 0.5, -0.5)
	# finally add the moves to the old positions and save the new position
	positions = positions + moves
}

```
</div>


<div class="right rt">
```{r}
hist(positions, breaks=40, col="gray", main="")
```

</div>




## Probability distributions: PDFs
<div class="left lt">
* We can rescale this histogram, such that the **area** of each bar represents the proportion of samples within that bin.

> - The height of each bar is the **probability density**.
> - The figure approximates an empirical **probability density function**.

</div>

<div class="right rt">
```{r}
hist(positions, breaks=40, col="gray", main = "", freq=FALSE)
```

</div>



## Probability distributions: PDFs
<div class="left lt">
* We can rescale this histogram, such that the **area** of each bar represents the proportion of samples within that bin.
* The height of each bar is the **probability density**.
* The figure approximates an empirical **probability density function**.
* We can use the `density` function in R to add a curve approximating this density. 

</div>

<div class="right rt">
```{r}
hist(positions, breaks=40, col="gray", main = "", freq=FALSE)
lines(density(positions, adjust=1.5), col='red', lwd=2)
```

</div>




## Normal distributions
<div class="left lt">
* This looks a lot like a normal distribution.
* As we take smaller and smaller steps, and do more coin flips, the distribution of values will converge exactly on a normal distribution.
* Here we add a normal **PDF** based on the sample mean and sd (in blue).

</div>

<div class="right rt">
```{r}
hist(positions, breaks=40, col="gray", main = "", freq=FALSE)
lines(density(positions, adjust=1.5), col='red', lwd=2)
mu = mean(positions)
sig = sd(positions)
x_norm = seq(min(positions), max(positions), length.out = 400)
y_norm = dnorm(x_norm, mu, sig)
lines(x_norm, y_norm, lwd=2, col='blue')
legend("topright", legend=c(paste("sample mean =", round(mu, 2)), 
                            paste("sample sd =", round(sig, 2))), lwd=0, bty='n')
```

</div>



## Normal distributions

> - In nature, any process that involves summing small random values produces exactly this shape.
> - The distribution is **symmetric** and **unimodal**.
> - mean, median, mode are all equal.
> - The distribution has two **parameters**, the mean ($\mu$) and the standard deviation ($\sigma$).
> - If $\mu = 0$ and $\sigma = 1$, the distribution is called the **standard normal**.
> - Transforming a variable so that $\mu = 0$ and $\sigma = 1$ is called **standardization**.
$$
x_{std} = \frac{(x - \bar{x})}{s}\approx\frac{(x - \mu)}{\sigma}
$$
> - In R, we can use the `scale` function.

## Normal distributions

* In nature, any process that involves summing small random values produces exactly this shape.
* The distribution is **symmetric** and **unimodal**.
* mean, median, mode are all equal.
* The distribution has two **parameters**, the mean ($\mu$) and the standard deviation ($\sigma$).
* If $\mu = 0$ and $\sigma = 1$, the distribution is called the **standard normal**.
* Transforming a variable so that $\mu = 0$ and $\sigma = 1$ is called **standardization**.
$$
x_{std} = \frac{(x - \bar{x})}{s}\approx\frac{(x - \mu)}{\sigma}
$$
* In R, we can use the `scale` function.



## Normal distributions

<div class="left lt">
* The typical "bell-curve" shape of the normal distribution is the Normal **Probability Density Function** (PDF)

$$
\mathcal{f}(x) = \frac{1}{\sigma \sqrt{2\pi}} \mathcal{e}^{-\frac{1}{2} \left (\frac{x-\mu}{\sigma} \right )^2}
$$

* It is easiest to understand what this function represents by understanding its **integral**.
* Recall that the area of each bar in our histogram represents the *proportion* of observations in that bin.


</div>

<div class="right rt">
```{r echo = FALSE}
hist(positions, breaks=40, col="gray", main = "", freq=FALSE)
lines(x_norm, y_norm, lwd=2, col='blue')
legend("topright", legend=c(paste("sample mean =", round(mu, 2)), 
                            paste("sample sd =", round(sig, 2))), lwd=0, bty='n')
```

</div>



## Normal distributions: area under the curve

<div class="left lt">

* The integral between two values $a$ and $b$ is equal to the area under the curve.
* This is exactly the probability of observing a value between $a$ and $b$.
* Rule of thumb: about 68% of values in any normal distribution will be within 1 standard deviation of the mean.


</div>

<div class="right rt">
```{r echo = FALSE}
plot(0,0, xlim=range(x_norm), ylim=range(y_norm), xlab = "X", ylab = "Probability Density", bty="n", type='n')
x_poly = seq(mu-sig, mu+sig, length.out=200)
y_poly = dnorm(x_poly, mean(positions), sd(positions))
x_poly = c(x_poly, rev(x_poly))
y_poly = c(y_poly, rep(0, length(y_poly)))
polygon(x_poly, y_poly, col=cols[2])
lines(x_norm, y_norm, lwd=2, col='blue')
text(mu+c(sig, -sig), dnorm(mu+c(sig, -sig), mu, sig), c(expression(mu+sigma), expression(mu-sigma)), pos = c(4,2))

legend("topright", legend=c(paste("sample mean =", round(mu, 2)), 
                            paste("sample sd =", round(sig, 2)),
                            paste("area under curve =", round(diff(pnorm(mu+c(-sig, sig), mu, sig)), 4))), lwd=0, bty='n')
```
</div>

## Normal distributions: area under the curve

<div class="left lt">

* The integral between two values $a$ and $b$ is equal to the area under the curve.
* This is exactly the probability of observing a value between $a$ and $b$.
* Rule of thumb: about 68% of values in any normal distribution will be within 1 standard deviation of the mean.
* Rule of thumb: 95% of values are $\mu \pm 1.96 \sigma \approx \mu \pm 2 \sigma$.

</div>

<div class="right rt">
```{r echo = FALSE}
plot(0,0, xlim=range(x_norm), ylim=range(y_norm), xlab = "X", ylab = "Probability Density", bty="n", type='n')
x_poly = seq(mu-1.96*sig, mu+1.96*sig, length.out=200)
y_poly = dnorm(x_poly, mean(positions), sd(positions))
x_poly = c(x_poly, rev(x_poly))
y_poly = c(y_poly, rep(0, length(y_poly)))
polygon(x_poly, y_poly, col=cols[2])
lines(x_norm, y_norm, lwd=2, col='blue')
text(mu+c(1.96*sig, -1.96*sig), dnorm(mu+c(1.96*sig, -1.96*sig), mu, sig), 
     c(expression(mu+1.96*sigma), expression(mu-1.96*sigma)), pos = c(4,2))

legend("topright", legend=c(paste("sample mean =", round(mu, 2)), 
                            paste("sample sd =", round(sig, 2)),
                            paste("area under curve =", round(diff(pnorm(mu+c(-1.96*sig, 1.96*sig), mu, sig)), 4))), lwd=0, bty='n')
```
</div>



## Normal distributions: CDF

<div class="left lt">
* If we integrate from  $-\infty$ to some value $x$, then we have the probability of observing a value **less than** $x$.
* This integral of the Normal PDF is known as the Normal **cumulative distribution function**.

$$
\mathcal{g}(x) = \int_{-\infty}^{x} \frac{1}{\sigma \sqrt{2\pi}} \mathcal{e}^{-\frac{1}{2} \left (\frac{x-\mu}{\sigma} \right )^2} dx
$$


```{r echo = FALSE}
plot(0,0, xlim=range(x_norm), ylim=c(0,1), xlab = "X", ylab = "Cumulative Probability", bty="n", type='n')
lines(x_norm, pnorm(x_norm, mu, sig), lwd=2, col='red')
```

</div>

<div class="right rt">


```{r echo = FALSE}
plot(0,0, xlim=range(x_norm), ylim=range(y_norm), xlab = "X", ylab = "Probability Density", bty="n", type='n')
x_poly = seq(-50, mu-1.96*sig, length.out=100)
y_poly = dnorm(x_poly, mean(positions), sd(positions))
x_poly = c(x_poly, rev(x_poly))
y_poly = c(y_poly, rep(0, length(y_poly)))
polygon(x_poly, y_poly, col=cols[2])
lines(x_norm, y_norm, lwd=2, col='blue')
text(mu+-1.96*sig, dnorm(mu-1.96*sig, mu, sig), expression(mu-1.96*sigma), pos = 4)

legend("topright", legend=c(paste("sample mean =", round(mu, 2)), 
                            paste("sample sd =", round(sig, 2)),
                            paste("area under curve =", round(pnorm(mu-1.96*sig, mu, sig), 4))), lwd=0, bty='n')
```


</div>


## Distributions in R

* R has four functions in common for many families of distributions:

**PDF**: what is the probability `d`ensity when $x=3$ (the height of the bell curve)

```{r}
dnorm(x = 3, mean = 0, sd = 1)
```

**CDF**: what is the cumulative `p`robability when $x=q$

(area under the bell curve from $-\infty$ to $q$)

(probability of observing a value < $q$)

```{r}
pnorm(q = -1.96, mean = 0, sd = 1)
```


**Quantiles**: what is the value of $x$, such that the probability of observing **x or smaller** is $p$

(inverse of the CDF)

```{r}
qnorm(p = 0.025, mean = 0, sd = 1)
```


**RNG**: Random number generator, produces $n$ random numbers from the desired distribution

```{r}
rnorm(n = 10, mean= 0, sd = 1)
```


R supports many distributions, we will discuss others as they come up.



## Sampling error
> - In practise, we work with samples, which are imperfectly representative of the populations from which they are drawn.
> - The **sample mean** $(\bar{x})$ and **sample standard deviation** $(s)$ approximate the population $\mu$ and $\sigma$.
> - We would like to known how good our estimates are!
> - A **standard error** tells you how well a **sample statistic** represents the **population parameter**.
> - If you repeat your sampling many times, and compute a separate sample statistic each time (e.g., $\bar{x}$, $s$, etc)...
> - The sample statistics will converge on a normal distribution (if the sample size is large).
> - The **standard deviation of sample statistics** will equal the **standard error** of the statistic.

## Standard error of the mean
* In practise, we work with samples, which are imperfectly representative of the populations from which they are drawn
* The **sample mean** $(\bar{x})$ and **sample standard deviation** $(s)$ approximate the population $\mu$ and $\sigma$
* We would like to known how good our estimates are!
* A **standard error** tells you how well a **sample statistic** represents the **population parameter**.
* If you repeat your sampling many times, and compute a separate sample statistic each time (e.g., $\bar{x}$, $s$, etc)...
* The sample statistics will converge on a normal distribution (if the sample size is large)
* The **standard deviation of sample statistics** will equal the **standard error** of the statistic.
* We commonly use the **standard error of the mean** to make inferences about the mean of a sample
* A larger sample size results in a smaller standard error


$$
\sigma_{\bar{x}} = \frac{\sigma}{\sqrt{n}} \approx \frac{s}{\sqrt{n}}
$$




## Central limit theorem
* If you repeat your sampling many times, and compute a separate sample statistic each time (e.g., $\bar{x}$, $s$, etc)...
* The sample statistics will converge on a normal distribution (if the sample size is large).

> - This holds generally in many natural systems.
> - If a variable $x$ is normally distributed, then $\bar{x}$ will be normally distributed, regardless of sample size.
> - The farther $x$ is away from a normal distribution, the larger $n$ must be before $\bar{x}$ follows a normal distribution.
> - If $n$ is large enough...
> - $\bar{x}$ will be normally distributed with a mean = $\mu$ and standard deviation = the standard error of the mean.