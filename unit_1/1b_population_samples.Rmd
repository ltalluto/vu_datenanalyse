---
title: "Populations & Samples"
author: "Matthew Talluto"
date: "13.02.2023"
output:
  slidy_presentation:
    theme: cerulean
    toc_depth: 2
    css: ../assets/rmd_style.css
  beamer_presentation: default
---


```{r setup, include=FALSE}
# knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.width=6.5, fig.height=6.5, collapse = TRUE, comment = "##", dev="png", error=TRUE, message = FALSE)
#knitr::opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
library(RColorBrewer)
cols = brewer.pal(8, "Set1")

library(knitr)
library(kableExtra)
options(digits = 3)
set.seed(12345)
data(penguins, package = "palmerpenguins")
penguins = as.data.frame(penguins)
penguins = penguins[complete.cases(penguins), ]

```

## Populations & Samples
<div class="left lt">
* A **population** is the unit about which we want to use statistics to describe, analyse, or perform inference.
  - e.g., all the trees in a forest
</div>

<div class="right rt">
![](img/forest1.png)
</div>


## Populations & Samples
<div class="left lt">
* A **population** is the unit about which we want to use statistics to describe, analyse, or perform inference.
  - e.g., all the trees in a forest
* A **sample** is a subset of a population that is observed with the goal of obtaining knowledge about a population.
</div>

<div class="right rt">
![](img/forest2.png)
</div>



## Populations & Samples
<div class="left lt">
* A **population** is the unit about which we want to use statistics to describe, analyse, or perform inference.
  - e.g., all the trees in a forest
* A **sample** is a subset of a population that is observed with the goal of obtaining knowledge about a population.
  - select 7 trees at random
  - Samples must be taken carefully so that they are representative of the entire population!
  - Gold standard: sample sizes must be **large** and drawn **randomly** from the population
  - **Experimental design**: designing the sample such that it represents the desired population
</div>

<div class="right rt">
![](img/forest2.png)
</div>


## Describing populations and samples
<div class="left lt">
* We can visualise the distribution of values within a population using a **histogram**
</div>

<div class="right rt">

```{r echo=FALSE, message=FALSE}
library(ggplot2)
# generate some random numbers from the lognormal distribution
my_var = rlnorm(1000, log(10), 0.5)

p1 = ggplot(data.frame(my_var), aes(x=my_var)) + geom_histogram() + ylab("Frequency") + xlab("Variable of Interest")
p1
```

</div>

## Aside: graphics in R

R currently has two dominant graphical engines

### Base graphics
* Built in to R.
* Simple to use
* Relatively easy syntax
* Plots usually need a lot of customization before they are finished

### ggplot2
* A package you must install.
* More complex syntax
* Produces publication-quality graphics with less customization needed.
* Plots can be saved in a variable, with customization added as needed.

We will mostly use base graphics in the course

If you want to learn ggplot, ask Matt

## Histograms: Base graphics
<div class="left lt">

* The workhorse function is `hist`

```{r eval = FALSE}
# generate some random numbers from the lognormal distribution
my_var = rlnorm(1000, log(10), 0.5)
hist(my_var)
```

</div>

<div class="right rt">
```{r echo = FALSE}
hist(my_var)
```
</div>


## Histograms: Base graphics

<div class="left lt">
* The workhorse function is `hist`

The defaults are not very nice, so lets improve things

* `main = ""` disables the title
* `xlab` and `ylab` control axis labels
* `breaks` controls the number of bins in the histogram
* `col` sets the color of the bars
* `border` sets the color of the borders (`NA`: no border)

```{r eval = FALSE}
hist(my_var, main = "", xlab="Variable of interest", ylab = "Frequency", 
	 breaks = 20, col="#999999", border=NA)
```

</div>

<div class="right rt">
```{r echo = FALSE}
hist(my_var, main = "", xlab="Variable of interest", ylab = "Frequency", 
	 breaks = 20, col="#999999", border=NA)
```

</div>

## Colors

<div class="left lt">
* R supports colors using the common HTML color coding: `#RRGGBB`
	- RR, GG, BB are the amounts of red, green, and blue
	- each ranges from `00` (none) to `FF` (most)
	- [Color pickers](https://www.w3schools.com/colors/colors_picker.asp) online help you translate a color in real life to a coded color
* R also has 657 named colors: `col = rosybrown`
* see the `colors()` function for the names

```{r}
head(colors())
```
</div>

<div class="right rt">

```{r echo = FALSE, fig.width=35, fig.height=18}
library(ggdark)
d = expand.grid(xl = 1:27, yb = 25:1)
d$colors = NA
d$colors[1:657] = colors()
d = d[complete.cases(d),]
par(mar = c(0,0,0,0))
plot(0,0, xlim = c(1,28), ylim = c(1,26), type = 'n', axes = FALSE, xlab = "", ylab = "")
with(d, rect(xl, yb, 1+xl, 1+yb, col = colors, border = NA))
with(d, text(xl, yb + 0.5, colors, col = invert_color(colors), cex = 1, adj = 0))
```

</div>





## Describing populations and samples
<div class="left lt">
* We can visualise the distribution of values within a population or sample using a **histogram**

> - We use **summary statistics** to describe the values numerically.
> - In practise, we must estimate population statistics via sample statistics.
> - We can formally describe the shape of the distribution of values using special statistics called **moments**.

</div>

<div class="right rt">
```{r echo = FALSE}
p1
```

</div>


## Summary statistics: location

<div class="left lt">

* **First moment**: Arithmetic mean. 

The population mean ($\mu$) can be approximated with the **sample mean**:

$$
\mu \approx \bar{x} = \frac{1}{n}\sum_{i=1}^n x_i
$$

</div>

<div class="right rt">
```{r echo=FALSE}
lwd=2
p2 = p1 + geom_vline(xintercept = mean(my_var), col = cols[1])
p2
```

```{r}
mean(my_var)
```

</div>



## Summary statistics: location

<div class="left lt">

* **First moment**: Arithmetic mean. 

### Other location statistics

The mean can be strongly influenced by outliers.

* median: The 50% quantile (i.e., half the values of x are above the median, half below)

</div>

<div class="right rt">
```{r echo=FALSE}
p2 = p2 + geom_vline(xintercept = median(my_var), col = cols[2])
p2
```

```{r}
median(my_var)

```

</div>



## Summary statistics: location

<div class="left lt">

* **First moment**: Arithmetic mean. 

### Other location statistics

The mean can be strongly influenced by outliers.

* median: The 50% quantile (i.e., half the values of x are above the median, half below)
* mode: the most common value, the "peak" of a distribution

</div>

<div class="right rt">
```{r echo=FALSE}
my_hist = hist(my_var, breaks = 30, plot = FALSE)
mode_position = which.max(my_hist$counts)
md = my_hist$mids[mode_position]

p2 = p2 + geom_vline(xintercept = median(md), col = cols[3])
p2
```

```{r}
# mode
mode(my_var) # wrong!

# we can use the histogram function to approximate the sample mode
# changing the number of breaks will have a large impact on the results
my_hist = hist(my_var, breaks = 30, plot = FALSE)

# the mids variable gives you the midpoint of each bin
# counts gives you the count each bin
# cbind shows them together in columns
head(cbind(my_hist$mids, my_hist$counts))

# use this to get the mode
# first find out which one is the tallest with which.max
(mode_position = which.max(my_hist$counts))
my_hist$mids[mode_position]
```

</div>






## Summary statistics: location

<div class="left lt">

* **First moment**: Arithmetic mean. 

### Other location statistics

* minimum, maximum values
* quantiles: the p-th quantile of x is the value of x such that p% of values are beneath the quantile
   - commonly used: (2.5%, 97.5%), 50%, (25%, 75%)

### Comparing variables

We can compare variables in a way that is *location independent* by **centering** (subtracting the mean)


</div>

<div class="right rt">

```{r}
c(min(my_var), max(my_var))
range(my_var)

quantile(my_var, 0.4)

# Centre the variable
# note! arithmetic operators like `-` are vectorized!
my_var_ctr = my_var - mean(my_var)
mean(my_var_ctr)

```

</div>





## Summary statistics: dispersion (or scale)

<div class="left lt">


* **Second moment**: Variance ($\sigma^2$)

$$
\sigma^2 = \frac{1}{N}\sum_{i=1}^N (X_i-\mu)^2
$$


We can estimate $\sigma^2$ using the **sample variance**:

$$
\sigma^2 \approx s^2 = \frac{1}{n-1}\sum_{i=1}^n (x_i -\bar{x})^2
$$

It is convenient to talk about the scale of $x$ in the same units as $x$ itself, so we use the (population or sample) **standard deviation**:

$$
\sigma = \sqrt{\sigma^2} \approx s = \sqrt{s^2}
$$


</div>
<div class="right rt">

```{r echo=FALSE}
sdata = data.frame(
	xm = mean(my_var),
	x = mean(my_var) + sd(my_var),
	xe = mean(my_var) - sd(my_var),
	y = 50
)
p3 = p1 + geom_point(data = sdata, aes(x=xm, y =y)) + 
	geom_segment(data = sdata, aes(x = x, xend = xe, y = y, yend = y), col = cols[1])
p3
```


```{r}
# Population variance -- biased if x is a sample!
sum((my_var - mean(my_var))^2)/length(my_var)

# These R functions always produce the sample variance and sd
var(my_var)
sd(my_var)


```
</div>



## Summary statistics: dispersion (or scale)

<div class="left lt">

### Other dispersion statistics

* range: `max(x) - min(x)`
* interquartile range (IQR): the difference between the third and first quartiles
* coefficient of variation (CV): 
$$
\frac{s}{|\bar{x}|}
$$


</div>
<div class="right rt">



```{r}
# the range function gives you the min and max
# Take the difference to get the statistical range
diff(range(my_var))
max(my_var) - min(my_var)

IQR(my_var)
quantile(my_var, 0.75) - quantile(my_var, 0.25)

# coefficient of variation can be done manually
sd(my_var)/mean(my_var)
```
</div>


## Summary statistics: dispersion (or scale)
<div class="left lt">

* A **boxplot** is a nice way to summarize location and dispersion in a dataset

</div>
<div class="right rt">

```{r}
boxplot(my_var)
```
</div>


## Summary statistics: higher moments
<div class="left lt">

### Asymmetry: skewness

Is the distribution weighted to one side or the other?

$$
\mathrm{skewness} = \frac{\sum_{i=1}^{n}(x_i-\bar{x})^3}{(n-1)s^3}
$$

```{r, echo = FALSE}
xx = seq(0, 1, length.out=100)
y1 = dbeta(xx, 2, 8)
y2 = dbeta(xx, 8, 2)
y3 = dbeta(xx, 5, 5)
y1 = y1/max(y1)
y2 = y2/max(y2)
y3 = y3/max(y3)

layout(matrix(2:1, ncol=1), heights = c(0.2, 1))
par(mar=c(2.5, 2.5, 0, 0.5), oma = c(0,0,0,0), mgp=c(1, 0.5, 0))
plot(xx, y3, type='l', col=cols[1], xaxt='n', yaxt='n', ylab="Density", lwd=2, xlab="Variable value", bty='l')
lines(xx, y1, type='l', col=cols[2], lwd=2)
lines(xx, y2, type='l', col=cols[3], lwd=2)
par(mar=c(0,0,0.5,0.5))
plot(0,0,type='n', axes=FALSE, xlab='', ylab='')
legend("topleft", legend = c("skew = 0", "right (positive) skew", "left (negative) skew"), cex = 0.8, lwd=2, bty='n', col=cols[1:3])

```

</div>
<div class="right rt">

### Tailedness: kurtosis

How fat are the tails relative to a normal distribution?

$$
\mathrm{kurtosis} = \frac{\sum_{i=1}^{n}(x_i-\bar{x})^4}{(n-1)s^4}
$$


```{r, echo = FALSE}
y1 = dnorm(xx, 0.5, 0.15)
y2 = dbeta(xx, 15, 15)
y3 = dbeta(xx, 3, 3)
# y1 = y1/max(y1)
# y2 = y2/max(y2)
# y3 = y3/max(y3)

# layout(matrix(1:2, nrow=1), widths = c(1, 0.5))
par(mar=c(2.5, 2.5, 0.5, 0), oma = c(0,0,0,0), mgp=c(1, 0.5, 0))
plot(xx, y1, type='l', col=cols[1], xaxt='n', yaxt='n', ylab="Density", lwd=2, xlab="Variable value", bty='l', ylim=c(0, max(y2)))
lines(xx, y2, type='l', col=cols[2], lwd=2)
lines(xx, y3, type='l', col=cols[3], lwd=2)
# par(mar=c(0,0,0.5,0.5))
# plot(0,0,type='n', axes=FALSE, xlab='', ylab='')
legend("topright", legend = c("mesokurtic (normal distribution)", 
                            "leptokurtic (positive kurtosis)", 
                            "platykurtic (negative kurtosis)"), lwd=2, bty='n', 
				col=cols[1:3], cex = 0.8)

```


</div>





## More on boxplots

<div class="left lt">

* **Boxplots** (sometimes **box-and-whisker** diagrams) summarize key statistics.
	- median
	- first and third quartiles (hinges)
	- approx. 95% confidence interval for median (notch)
	- min/max (or quartile + 1.5*IQR) (whiskers)
* They are very useful for comparing variables among groups.
* Base graphics: we use the `boxplot` function
* `y ~ group` is a special data type called a `formula`

</div>

<div class="right rt">

```{r}
boxplot(body_mass_g ~ species, data = penguins, boxwex=0.4, notch = TRUE)
```


</div>


## More on boxplots

<div class="left lt">

* **Boxplots** (sometimes **box-and-whisker** diagrams) summarize key statistics.
	- median
	- first and third quartiles (hinges)
	- approx. 95% confidence interval for median (notch)
	- min/max (or quartile + 1.5*IQR) (whiskers)
* They are very useful for comparing variables among groups.
* Base graphics: we use the `boxplot` function
* `y ~ group` is a special data type called a `formula`

</div>

<div class="right rt">

```{r, eval = FALSE}
boxplot(body_mass_g ~ species, data = penguins, boxwex=0.4, notch = TRUE)
```

```{r, echo = FALSE}
boxplot(body_mass_g ~ species, data = penguins, boxwex=0.4, notch = TRUE)
rng = range(penguins$body_mass_g[penguins$species == "Adelie"])
lines(c(0.75, 0.75), rng, col=cols[1])
text(0.75, rng[2], "range", col=cols[1], pos=2)

iqr = quantile(penguins$body_mass_g[penguins$species == "Chinstrap"], c(0.25, 0.75))
lines(c(1.75, 1.75), iqr, col=cols[2])
text(1.75, iqr[2], "IQR", col=cols[2], pos=2)
med = median(penguins$body_mass_g[penguins$species == "Chinstrap"])
text(2.15, med, "median", col=cols[3], pos=4)

lines(c(2.75, 2.75), c(4950, 5150), col=cols[4])
text(2.75, 4950, "~95 % CI", col = cols[4], pos = 2)

text(1.7,5700, 
     "outliers\n x < quartile(x, 0.25) - 1.5*IQR(x)\n x > quartile(x, 0.75) + 1.5*IQR(x)", 
     pos=1, col=cols[5], cex=0.7)
lines(c(1.8, 2), c(5300, 4850), col = cols[5])
```

</div>




## Repeating yourself

<div class="left lt">
* An important programming concept is **DRY**: Don't Repeat Yourself!
   - Don't copy and paste code if you can instead instruct the computer to repeat things for you.
* There are many ways to execute the same code repeatedly; we will cover these in detail as they are needed.
* A motivating example:
    - Compute the mean of every numeric variable in the `penguins` dataset

</div>

<div class="right rt">

```{r}
# bad!
(col_means = c(
  mean(penguins[,3]),
  mean(penguins[,4]),
  mean(penguins[,5]),
  mean(penguins[,3])  # it's easy to introduce mistakes this way!
))
```

</div>



## Applying functions

* We want to **apply** the function named `mean` separately to every variable in `penguins` 
* R has many different ways to apply functions, in this case we use `s`imple `apply` (`sapply`)

```{r}
# bad!
(col_means = c(
  mean(penguins[,3]),
  mean(penguins[,4]),
  mean(penguins[,5]),
  mean(penguins[,3])  # it's easy to introduce mistakes this way!
))

# better
# use columns 3:6 because these are the numeric columns
# doesn't make sense to take the mean of penguins$species
(col_means = sapply(penguins[,3:6], mean)) # apply the function mean to each variable
```



## Applying functions

* We want to **apply** the function named `mean` separately to every variable in `penguins` 
* R has many different ways to apply functions, in this case we use `s`imple `apply` (`sapply`)
* this works with any function that accepts a vector, even when they return more than a single value
```{r}
sapply(penguins[,3:6], range)
# here, we pass an additional argument named probs to quantile
# see ?quantile for what this does
sapply(penguins[,3:6], quantile, probs = c(0.25, 0.5, 0.75))
```







## Tabular applies

<div class="left lt">
* There are three species of penguin; it makes more sense to have summary statistics for each one
* `tapply`: `t`abular `apply`
  - produces a summary table for a single variable (e.g., `bill_length_mm`) based on categories in another variable (e.g., `species`)

</div>

<div class="right rt">

```{r}
# with: use the variables found in the data.frame penguins
# allows us to not write penguins$species to access the variable species
with(penguins, 
	 # compute the mean of bill_length_mm for each of the three species
	 tapply(bill_length_mm, species, mean)
)
```

</div>




## Probability distributions

<div class="left lt">

**Thought experiment**

You and 99 of your closest friends gather on the centre line of a 100-m football pitch. We define the centre line as 0m, the western boundary -50 m, and the eastern boundary as +50 m.

</div>

<div class="right rt">

```{r echo=FALSE, fig.height=4}
library("plotrix")
draw_pitch = function() {
	grn = "#39A83B"
	xl = c(-50, 50)
	yl = c(0, 66)
	par(mar = c(2,0,0,0))
	plot(0,0, xlim=xl+c(-5, 5), ylim = yl+c(-5, 5), type='n', axes = FALSE, ylab = '', xlab = '', bty='n', asp=1)
	axis(1, seq(xl[1], xl[2], 25))
	rect(xl[1]-5, yl[1]-5, xl[2]+5, yl[2]+5, col = grn, border = 'white')
	rect(xl[1], yl[1], xl[2], yl[2], border = 'white')
	
	draw.arc(rep(xl, each=2), rep(yl, 2), 0.5, deg1=c(0, 270, 90, 180), deg2 = c(90, 360, 180, 270), col = 'white')
	
	
	draw.circle(xl[1]+11, median(yl), 9.15, border = 'white')
	draw.circle(xl[2]-11, median(yl), 9.15, border = 'white')
	draw.circle(median(xl), median(yl), 9.15, border = 'white')
	lines(c(0,0), yl, col='white')
	# penalty boxes
	pby = median(yl) + c(-1, 1)*(40.3/2)
	rect(xl[1], pby[1], xl[1]+16.5, pby[2], col = grn, border = 'white')
	rect(xl[2]-16.5, pby[1], xl[2], pby[2], col = grn, border = 'white')
	# goal boxes
	gby = median(yl) + c(-10, 10)
	rect(xl[1], gby[1], xl[1]+5.5, gby[2], col = grn, border = 'white')
	rect(xl[2]-5.5, gby[1], xl[2], gby[2], col = grn, border = 'white')
}
draw_players = function(px, py) {
	points(px, py, pch = 16, col="black", cex=0.5)
}
draw_pitch()
ply = seq(1, 65, length.out = 100)
plx = rep(0, 100)
draw_players(plx, ply)

```

```{r, echo = FALSE, warning = FALSE}
ggplot(data.frame(plx=plx), aes(x=plx)) + geom_histogram(bins=40) + xlab("Player positions") + xlim(-50, 50)
```

</div>





## Probability distributions
<div class="left lt">

**Thought experiment**

You and 99 of your closest friends gather on the centre line of a 100-m football pitch. We define the centre line as 0m, the western boundary -50 m, and the eastern boundary as +50 m.

Each person flips a fair coin. If the coin is heads, they take a step east (add 0.5 m to their location), if its tails, they take a step west (subtract 0.5 m from their location). 

**Question**: What is the long-run distribution of positions on the field?


</div>

<div class="right rt">

```{r echo=FALSE, fig.height=4}
draw_pitch()
plx = plx + sample(c(-0.5, 0.5), length(plx), replace = TRUE)
draw_players(plx, ply)

```

```{r, echo = FALSE, warning = FALSE}
ggplot(data.frame(plx=plx), aes(x=plx)) + geom_histogram(bins=40) + xlab("Player positions") + xlim(-50, 50)
```

</div>





## Probability distributions
<div class="left lt">

**Thought experiment**

You and 99 of your closest friends gather on the centre line of a 100-m football pitch. We define the centre line as 0m, the western boundary -50 m, and the eastern boundary as +50 m.

Each person flips a fair coin. If the coin is heads, they take a step east (add 0.5 m to their location), if its tails, they take a step west (subtract 0.5 m from their location). 

**Question**: What is the long-run distribution of positions on the field?

t = 10

</div>

<div class="right rt">

```{r echo=FALSE, fig.height=4}
draw_pitch()
for(i in 1:9)
	plx = plx + sample(c(-0.5, 0.5), length(plx), replace = TRUE)
draw_players(plx, ply)

```

```{r, echo = FALSE, warning = FALSE}
ggplot(data.frame(plx=plx), aes(x=plx)) + geom_histogram(bins=40) + xlab("Player positions") + xlim(-50, 50)
```

</div>





## Probability distributions
<div class="left lt">

**Thought experiment**

You and 99 of your closest friends gather on the centre line of a 100-m football pitch. We define the centre line as 0m, the western boundary -50 m, and the eastern boundary as +50 m.

Each person flips a fair coin. If the coin is heads, they take a step east (add 0.5 m to their location), if its tails, they take a step west (subtract 0.5 m from their location). 

**Question**: What is the long-run distribution of positions on the field?

t = 50

</div>

<div class="right rt">

```{r echo=FALSE, fig.height=4}
draw_pitch()
for(i in 1:40)
	plx = plx + sample(c(-0.5, 0.5), length(plx), replace = TRUE)
draw_players(plx, ply)

```

```{r, echo = FALSE, warning = FALSE}
ggplot(data.frame(plx=plx), aes(x=plx)) + geom_histogram(bins=40) + xlab("Player positions") + xlim(-50, 50)
```

</div>




## Probability distributions
<div class="left lt">

**Thought experiment**

You and 99 of your closest friends gather on the centre line of a 100-m football pitch. We define the centre line as 0m, the western boundary -50 m, and the eastern boundary as +50 m.

Each person flips a fair coin. If the coin is heads, they take a step east (add 0.5 m to their location), if its tails, they take a step west (subtract 0.5 m from their location). 

**Question**: What is the long-run distribution of positions on the field?

t = 500

</div>

<div class="right rt">

```{r echo=FALSE, fig.height=4}
draw_pitch()
for(i in 1:450)
	plx = plx + sample(c(-0.5, 0.5), length(plx), replace = TRUE)
draw_players(plx, ply)

```

```{r, echo = FALSE, warning = FALSE}
ggplot(data.frame(plx=plx), aes(x=plx)) + geom_histogram(bins=40) + xlab("Player positions") + xlim(-50, 50)
```

</div>






## Probability distributions

We can simulate this experiment in R. Here is code for doing it for one person:

```{r}
position = 0
for(i in 1:100) {
  coin_flip = sample(c("heads", "tails"), 1)
  if(coin_flip == "heads") {
    position = position + 0.5
  } else {
    position = position - 0.5
  }
}
position
```



## Probability distributions
<div class="left lt">

And for a larger sample (2500) friends, doing more coin flips (500 each), with more concise code.

```{r include=FALSE}
set.seed(123)
```

```{r, results=FALSE}
# repeat the initial position 2500 times, one per friend
n_friends = 2500
positions = rep(0, n_friends)

# repeat the experiment 500 times
time_steps = 1:500
for(i in time_steps) {
	# flip a coin for each friend
	# returns one heads or tails for each n_friends
	coin_flips = sample(c("heads", "tails"), size = n_friends, replace = TRUE)
	# compute the steps: +0.5 if the flip is heads, -0.5 if tails
	moves = ifelse(coin_flips == "heads", 0.5, -0.5)
	# finally add the moves to the old positions and save the new position
	positions = positions + moves
}

```
</div>


<div class="right rt">
```{r}
hist(positions, breaks=40, col="gray", main="")
```

</div>




## Probability distributions: PDFs
<div class="left lt">
* We can rescale this histogram, such that the **area** of each bar represents the proportion of samples within that bin.

> - The height of each bar is the **probability density**.
> - The figure approximates an empirical **probability density function**.

</div>

<div class="right rt">
```{r}
hist(positions, breaks=40, col="gray", main = "", freq=FALSE)
```

</div>



## Probability distributions: PDFs
<div class="left lt">
* We can rescale this histogram, such that the **area** of each bar represents the proportion of samples within that bin.
* The height of each bar is the **probability density**.
* The figure approximates an empirical **probability density function**.
* We can use the `density` function in R to add a curve approximating this density. 

</div>

<div class="right rt">
```{r}
hist(positions, breaks=40, col="gray", main = "", freq=FALSE)
lines(density(positions, adjust=1.5), col='red', lwd=2)
```

</div>




## Normal distributions
<div class="left lt">
* This looks a lot like a normal distribution.
* As we take smaller and smaller steps, and do more coin flips, the distribution of values will converge exactly on a normal distribution.
* Here we add a normal **PDF** based on the sample mean and sd (in blue).

</div>

<div class="right rt">
```{r}
hist(positions, breaks=40, col="gray", main = "", freq=FALSE)
lines(density(positions, adjust=1.5), col='red', lwd=2)
mu = mean(positions)
sig = sd(positions)
x_norm = seq(min(positions), max(positions), length.out = 400)
y_norm = dnorm(x_norm, mu, sig)
lines(x_norm, y_norm, lwd=2, col='blue')
legend("topright", legend=c(paste("sample mean =", round(mu, 2)), 
                            paste("sample sd =", round(sig, 2))), lwd=0, bty='n')
```

</div>



## Normal distributions

> - In nature, any process that involves summing small random values produces exactly this shape.
> - The distribution is **symmetric** and **unimodal**.
> - mean, median, mode are all equal.
> - The distribution has two **parameters**, the mean ($\mu$) and the standard deviation ($\sigma$).
> - If $\mu = 0$ and $\sigma = 1$, the distribution is called the **standard normal**.
> - Transforming a variable so that $\mu = 0$ and $\sigma = 1$ is called **standardization**.
$$
x_{std} = \frac{(x - \bar{x})}{s}\approx\frac{(x - \mu)}{\sigma}
$$
> - In R, we can use the `scale` function.

## Normal distributions

* In nature, any process that involves summing small random values produces exactly this shape.
* The distribution is **symmetric** and **unimodal**.
* mean, median, mode are all equal.
* The distribution has two **parameters**, the mean ($\mu$) and the standard deviation ($\sigma$).
* If $\mu = 0$ and $\sigma = 1$, the distribution is called the **standard normal**.
* Transforming a variable so that $\mu = 0$ and $\sigma = 1$ is called **standardization**.
$$
x_{std} = \frac{(x - \bar{x})}{\sigma}
$$
* In R, we can use the `scale` function.



## Normal distributions

<div class="left lt">
* The typical "bell-curve" shape of the normal distribution is the Normal **Probability Density Function** (PDF)

$$
\mathcal{f}(x) = \frac{1}{\sigma \sqrt{2\pi}} \mathcal{e}^{-\frac{1}{2} \left (\frac{x-\mu}{\sigma} \right )^2}
$$

* It is easiest to understand what this function represents by understanding its **integral**.
* Recall that the area of each bar in our histogram represents the *proportion* of observations in that bin.


</div>

<div class="right rt">
```{r echo = FALSE}
hist(positions, breaks=40, col="gray", main = "", freq=FALSE)
lines(x_norm, y_norm, lwd=2, col='blue')
legend("topright", legend=c(paste("sample mean =", round(mu, 2)), 
                            paste("sample sd =", round(sig, 2))), lwd=0, bty='n')
```

</div>



## Normal distributions: area under the curve

<div class="left lt">

* The integral between two values $a$ and $b$ is equal to the area under the curve.
* This is exactly the probability of observing a value between $a$ and $b$.
* Rule of thumb: about 68% of values in any normal distribution will be within 1 standard deviation of the mean.


</div>

<div class="right rt">
```{r echo = FALSE}
plot(0,0, xlim=range(x_norm), ylim=range(y_norm), xlab = "X", ylab = "Probability Density", bty="n", type='n')
x_poly = seq(mu-sig, mu+sig, length.out=200)
y_poly = dnorm(x_poly, mean(positions), sd(positions))
x_poly = c(x_poly, rev(x_poly))
y_poly = c(y_poly, rep(0, length(y_poly)))
polygon(x_poly, y_poly, col=cols[2])
lines(x_norm, y_norm, lwd=2, col='blue')
text(mu+c(sig, -sig), dnorm(mu+c(sig, -sig), mu, sig), c(expression(mu+sigma), expression(mu-sigma)), pos = c(4,2))

legend("topright", legend=c(paste("sample mean =", round(mu, 2)), 
                            paste("sample sd =", round(sig, 2)),
                            paste("area under curve =", round(diff(pnorm(mu+c(-sig, sig), mu, sig)), 4))), lwd=0, bty='n')
```
</div>

## Normal distributions: area under the curve

<div class="left lt">

* The integral between two values $a$ and $b$ is equal to the area under the curve.
* This is exactly the probability of observing a value between $a$ and $b$.
* Rule of thumb: about 68% of values in any normal distribution will be within 1 standard deviation of the mean.
* Rule of thumb: 95% of values are $\mu \pm 1.96 \sigma \approx \mu \pm 2 \sigma$.

</div>

<div class="right rt">
```{r echo = FALSE}
plot(0,0, xlim=range(x_norm), ylim=range(y_norm), xlab = "X", ylab = "Probability Density", bty="n", type='n')
x_poly = seq(mu-1.96*sig, mu+1.96*sig, length.out=200)
y_poly = dnorm(x_poly, mean(positions), sd(positions))
x_poly = c(x_poly, rev(x_poly))
y_poly = c(y_poly, rep(0, length(y_poly)))
polygon(x_poly, y_poly, col=cols[2])
lines(x_norm, y_norm, lwd=2, col='blue')
text(mu+c(1.96*sig, -1.96*sig), dnorm(mu+c(1.96*sig, -1.96*sig), mu, sig), 
     c(expression(mu+1.96*sigma), expression(mu-1.96*sigma)), pos = c(4,2))

legend("topright", legend=c(paste("sample mean =", round(mu, 2)), 
                            paste("sample sd =", round(sig, 2)),
                            paste("area under curve =", round(diff(pnorm(mu+c(-1.96*sig, 1.96*sig), mu, sig)), 4))), lwd=0, bty='n')
```
</div>



## Normal distributions: CDF

<div class="left lt">
* If we integrate from  $-\infty$ to some value $x$, then we have the probability of observing a value **less than** $x$.
* This integral of the Normal PDF is known as the Normal **cumulative distribution function**.

$$
\mathcal{g}(x) = \int_{-\infty}^{x} \frac{1}{\sigma \sqrt{2\pi}} \mathcal{e}^{-\frac{1}{2} \left (\frac{x-\mu}{\sigma} \right )^2} dx
$$


```{r echo = FALSE}
plot(0,0, xlim=range(x_norm), ylim=c(0,1), xlab = "X", ylab = "Cumulative Probability", bty="n", type='n')
lines(x_norm, pnorm(x_norm, mu, sig), lwd=2, col='red')
```

</div>

<div class="right rt">


```{r echo = FALSE}
plot(0,0, xlim=range(x_norm), ylim=range(y_norm), xlab = "X", ylab = "Probability Density", bty="n", type='n')
x_poly = seq(-50, mu-1.96*sig, length.out=100)
y_poly = dnorm(x_poly, mean(positions), sd(positions))
x_poly = c(x_poly, rev(x_poly))
y_poly = c(y_poly, rep(0, length(y_poly)))
polygon(x_poly, y_poly, col=cols[2])
lines(x_norm, y_norm, lwd=2, col='blue')
text(mu+-1.96*sig, dnorm(mu-1.96*sig, mu, sig), expression(mu-1.96*sigma), pos = 4)

legend("topright", legend=c(paste("sample mean =", round(mu, 2)), 
                            paste("sample sd =", round(sig, 2)),
                            paste("area under curve =", round(pnorm(mu-1.96*sig, mu, sig), 4))), lwd=0, bty='n')
```


</div>


## Distributions in R

* R has four functions in common for many families of distributions:

**PDF**: what is the probability `d`ensity when $x=3$ (the height of the bell curve)

```{r}
dnorm(x = 3, mean = 0, sd = 1)
```

**CDF**: what is the cumulative `p`robability when $x=q$

(area under the bell curve from $-\infty$ to $q$)

(probability of observing a value < $q$)

```{r}
pnorm(q = -1.96, mean = 0, sd = 1)
```


**Quantiles**: what is the value of $x$, such that the probability of observing **x or smaller** is $p$

(inverse of the CDF)

```{r}
qnorm(p = 0.025, mean = 0, sd = 1)
```


**RNG**: Random number generator, produces $n$ random numbers from the desired distribution

```{r}
rnorm(n = 10, mean= 0, sd = 1)
```


R supports many distributions, we will discuss others as they come up.



## Sampling error
> - In practise, we work with samples, which are imperfectly representative of the populations from which they are drawn.
> - The **sample mean** $(\bar{x})$ and **sample standard deviation** $(s)$ approximate the population $\mu$ and $\sigma$.
> - We would like to known how good our estimates are!
> - A **standard error** tells you how well a **sample statistic** represents the **population parameter**.
> - If you repeat your sampling many times, and compute a separate sample statistic each time (e.g., $\bar{x}$, $s$, etc)...
> - The sample statistics will converge on a normal distribution (if the sample size is large).
> - The **standard deviation of sample statistics** will equal the **standard error** of the statistic.

## Standard error of the mean
* In practise, we work with samples, which are imperfectly representative of the populations from which they are drawn
* The **sample mean** $(\bar{x})$ and **sample standard deviation** $(s)$ approximate the population $\mu$ and $\sigma$
* We would like to known how good our estimates are!
* A **standard error** tells you how well a **sample statistic** represents the **population parameter**.
* If you repeat your sampling many times, and compute a separate sample statistic each time (e.g., $\bar{x}$, $s$, etc)...
* The sample statistics will converge on a normal distribution (if the sample size is large)
* The **standard deviation of sample statistics** will equal the **standard error** of the statistic.
* We commonly use the **standard error of the mean** to make inferences about the mean of a sample
* A larger sample size results in a smaller standard error


$$
\sigma_{\bar{x}} = \frac{\sigma}{\sqrt{n}} \approx \frac{s}{\sqrt{n}}
$$




## Central limit theorem
* If you repeat your sampling many times, and compute a separate sample statistic each time (e.g., $\bar{x}$, $s$, etc)...
* The sample statistics will converge on a normal distribution (if the sample size is large).

> - This holds generally in many natural systems.
> - If a variable $x$ is normally distributed, then $\bar{x}$ will be normally distributed, regardless of sample size.
> - The farther $x$ is away from a normal distribution, the larger $n$ must be before $\bar{x}$ follows a normal distribution.
> - If $n$ is large enough...
> - $\bar{x}$ will be normally distributed with a mean = $\mu$ and standard deviation = the standard error of the mean.